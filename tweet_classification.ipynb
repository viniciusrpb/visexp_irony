{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObVElOqfG7WnZteyzRzDWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viniciusrpb/visexp_irony/blob/master/tweet_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr9M0E3P2oA2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "dd923ad7-703f-4f6b-9525-219f7f4bdef6"
      },
      "source": [
        "import nltk\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.datasets import make_classification\n",
        "import sklearn.feature_extraction.text\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def num_there(s):\n",
        "    return any(i.isdigit() for i in s)\n",
        "\n",
        "file = open('SemEval2017-task4-test.subtask-A.english.txt','r',encoding =\"utf8\")\n",
        "tweets = []\n",
        "tfidf = []\n",
        "buffer = []\n",
        "while True:\n",
        "    # read line\n",
        "    line = file.readline()\n",
        "    if line:\n",
        "        tweetsJustRead= line.split(\"\\t\",2)\n",
        "        buffer = [tweetsJustRead[2]] \n",
        "        \n",
        "        vectorizer = TfidfVectorizer(norm=None, stop_words=\"english\")\n",
        "        \n",
        "        words_vector = vectorizer.fit_transform(buffer)\n",
        "        tfidf.append(words_vector)\n",
        "        tweets.append(buffer)\n",
        "\n",
        "        \n",
        "    if not line:\n",
        "        break\n",
        "\n",
        "file.close()\n",
        "\n",
        "data = {'TF-IDF':tweets,\n",
        "        'Tweets':words_vector}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  TF-IDF                                             Tweets\n",
            "0      [#ArianaGrande Ari By Ariana Grande 80% Full h...    (0, 13)\\t1.0\\n  (0, 4)\\t1.0\\n  (0, 8)\\t1.0\\n...\n",
            "1      [Ariana Grande KIIS FM Yours Truly CD listenin...    (0, 13)\\t1.0\\n  (0, 4)\\t1.0\\n  (0, 8)\\t1.0\\n...\n",
            "2      [Ariana Grande White House Easter Egg Roll in ...    (0, 13)\\t1.0\\n  (0, 4)\\t1.0\\n  (0, 8)\\t1.0\\n...\n",
            "3      [#CD #Musics Ariana Grande Sweet Like Candy 3....    (0, 13)\\t1.0\\n  (0, 4)\\t1.0\\n  (0, 8)\\t1.0\\n...\n",
            "4      [SIDE TO SIDE ðŸ˜˜ @arianagrande #sidetoside #ari...    (0, 13)\\t1.0\\n  (0, 4)\\t1.0\\n  (0, 8)\\t1.0\\n...\n",
            "...                                                  ...                                                ...\n",
            "12279  [@dansen17 update: Zac Efron kissing a puppy h...    (0, 13)\\t1.0\\n  (0, 4)\\t1.0\\n  (0, 8)\\t1.0\\n...\n",
            "12280  [#zac efron sex pic skins michelle sex https:/...    (0, 13)\\t1.0\\n  (0, 4)\\t1.0\\n  (0, 8)\\t1.0\\n...\n",
            "12281  [First Look at Neighbors 2 with Zac Efron Shir...    (0, 13)\\t1.0\\n  (0, 4)\\t1.0\\n  (0, 8)\\t1.0\\n...\n",
            "12282  [zac efron poses nude #lovely libra porn https...    (0, 13)\\t1.0\\n  (0, 4)\\t1.0\\n  (0, 8)\\t1.0\\n...\n",
            "12283  [#Fashion #Style The Paperboy (NEW Blu-ray Dis...    (0, 13)\\t1.0\\n  (0, 4)\\t1.0\\n  (0, 8)\\t1.0\\n...\n",
            "\n",
            "[12284 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}